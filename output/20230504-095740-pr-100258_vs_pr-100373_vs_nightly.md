Description:
- 20230503-183025-pr
Torch version: 2.1.0a0+git5e1bf10
Torch config: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_61,code=sm_61
  - CuDNN 8.5
  - Build settings: BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_PYTORCH_QNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=1, USE_CUDNN=1, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=0, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=0, USE_OPENMP=ON, USE_ROCM=OFF, 


- 20230504-094423-pr-100373
Torch version: 2.1.0a0+git5e1bf10
Torch config: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_61,code=sm_61
  - CuDNN 8.5
  - Build settings: BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_PYTORCH_QNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=1, USE_CUDNN=1, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=0, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=0, USE_OPENMP=ON, USE_ROCM=OFF, 


- 20230503-184900-nightly
Torch version: 2.1.0a0+git2b75955
Torch config: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - CPU capability usage: AVX2
  - Build settings: BUILD_TYPE=Release, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_PYTORCH_QNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=0, USE_CUDNN=OFF, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=0, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=0, USE_OPENMP=ON, USE_ROCM=OFF, 



[----------------------------------------------------------------------------------------------------- Resize ----------------------------------------------------------------------------------------------------]
                                                                                                   |  torch (2.1.0a0+git5e1bf10) PR  |  torch (2.1.0a0+git5e1bf10) PR-100373  |  torch (2.1.0a0+git2b75955) nightly
1 threads: --------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (32, 32) aa=True       |         60.601 (+-0.106)        |            55.212 (+-0.155)            |           58.284 (+-0.435)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (32, 32) aa=False      |         41.510 (+-0.105)        |            36.527 (+-0.195)            |           38.125 (+-0.696)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (224, 224) aa=True     |        239.318 (+-0.818)        |           152.154 (+-0.556)            |          153.347 (+-1.132)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (224, 224) aa=False    |        228.668 (+-0.761)        |           140.325 (+-0.594)            |          144.072 (+-1.315)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (320, 320) aa=True     |        378.715 (+-0.597)        |           208.298 (+-0.439)            |          210.292 (+-1.617)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (320, 320) aa=False    |        376.553 (+-1.090)        |           206.339 (+-0.551)            |          210.428 (+-1.582)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (520, 520) -> (32, 32) aa=True       |        137.584 (+-0.765)        |           129.158 (+-0.868)            |          129.612 (+-1.347)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (520, 520) -> (32, 32) aa=False      |         62.716 (+-0.225)        |            56.442 (+-0.075)            |           58.779 (+-0.249)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (520, 520) -> (224, 224) aa=True     |        422.503 (+-4.381)        |           320.006 (+-0.777)            |          323.681 (+-1.891)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (520, 520) -> (224, 224) aa=False    |        330.370 (+-0.974)        |           231.621 (+-0.651)            |          236.182 (+-1.278)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (712, 712) -> (32, 32) aa=True       |        210.268 (+-1.614)        |           202.450 (+-0.951)            |          201.925 (+-1.383)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (712, 712) -> (32, 32) aa=False      |         81.845 (+-0.287)        |            74.968 (+-0.777)            |           77.452 (+-0.332)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (712, 712) -> (224, 224) aa=True     |        551.660 (+-1.154)        |           440.872 (+-0.934)            |          449.032 (+-2.235)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (712, 712) -> (224, 224) aa=False    |        410.742 (+-0.517)        |           303.562 (+-1.733)            |          308.009 (+-1.750)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (64, 64) -> (224, 224) aa=True       |        158.684 (+-0.696)        |            76.948 (+-0.344)            |           78.547 (+-0.327)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (224, 224) -> (270, 268) aa=True     |        280.466 (+-0.432)        |           159.500 (+-1.325)            |          161.176 (+-1.525)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (1024, 1024) aa=True   |        2545.385 (+-7.150)       |           880.101 (+-3.257)            |          886.613 (+-32.962)        
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (224, 224) -> (64, 64) aa=True       |         80.679 (+-0.535)        |            69.046 (+-1.531)            |           72.344 (+-0.389)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (270, 268) -> (224, 224) aa=True     |        251.536 (+-0.804)        |           164.413 (+-0.530)            |          165.781 (+-2.078)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (1024, 1024) -> (256, 256) aa=True   |        895.423 (+-19.556)       |           757.234 (+-4.775)            |          772.582 (+-3.169)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (64, 64) -> (224, 224) aa=False      |        155.591 (+-1.004)        |            74.790 (+-0.399)            |           77.902 (+-0.212)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (224, 224) -> (270, 268) aa=False    |        277.721 (+-0.730)        |           156.984 (+-0.384)            |          160.810 (+-1.141)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (1024, 1024) aa=False  |        2538.470 (+-6.667)       |           873.428 (+-3.844)            |         1067.035 (+-116.922)       
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (224, 224) -> (64, 64) aa=False      |         61.592 (+-0.096)        |            50.668 (+-0.102)            |           52.721 (+-0.336)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (270, 268) -> (224, 224) aa=False    |        235.333 (+-0.720)        |           146.364 (+-0.294)            |          150.042 (+-1.116)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (1024, 1024) -> (256, 256) aa=False  |        614.462 (+-0.730)        |           471.141 (+-0.926)            |          474.922 (+-2.858)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (32, 32) aa=True       |         55.171 (+-0.143)        |            50.849 (+-0.090)            |           53.613 (+-0.257)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (32, 32) aa=False      |         38.034 (+-0.099)        |            33.491 (+-0.074)            |           36.511 (+-0.207)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (224, 224) aa=True     |        234.656 (+-0.513)        |           145.938 (+-0.519)            |          145.831 (+-1.514)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (224, 224) aa=False    |        223.355 (+-0.762)        |           133.912 (+-0.766)            |          132.796 (+-2.350)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (320, 320) aa=True     |        384.612 (+-0.980)        |           204.058 (+-1.270)            |          198.385 (+-1.591)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (320, 320) aa=False    |        383.820 (+-0.891)        |           200.964 (+-1.106)            |          194.963 (+-1.050)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (520, 520) -> (32, 32) aa=True       |        125.552 (+-0.364)        |           124.150 (+-1.131)            |          125.838 (+-1.494)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (520, 520) -> (32, 32) aa=False      |         59.145 (+-0.085)        |            55.112 (+-0.066)            |           58.044 (+-0.621)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (520, 520) -> (224, 224) aa=True     |        375.413 (+-0.584)        |           286.061 (+-0.770)            |          287.312 (+-2.086)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (520, 520) -> (224, 224) aa=False    |        305.229 (+-0.964)        |           218.026 (+-0.641)            |          214.005 (+-1.278)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (712, 712) -> (32, 32) aa=True       |        194.075 (+-0.572)        |           192.990 (+-1.980)            |          193.942 (+-1.930)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (712, 712) -> (32, 32) aa=False      |         76.552 (+-0.263)        |            72.393 (+-0.182)            |           75.659 (+-0.272)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (712, 712) -> (224, 224) aa=True     |        512.977 (+-0.663)        |           421.066 (+-1.086)            |          423.461 (+-2.508)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (712, 712) -> (224, 224) aa=False    |        363.687 (+-0.640)        |           279.421 (+-0.628)            |          273.959 (+-1.750)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (64, 64) -> (224, 224) aa=True       |        167.617 (+-0.415)        |            76.130 (+-0.241)            |           78.373 (+-0.281)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (224, 224) -> (270, 268) aa=True     |        283.008 (+-0.849)        |           154.236 (+-1.446)            |          153.770 (+-1.025)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (1024, 1024) aa=True   |        2901.766 (+-5.261)       |           919.286 (+-8.762)            |          907.133 (+-7.998)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (224, 224) -> (64, 64) aa=True       |         75.712 (+-0.439)        |            66.253 (+-0.154)            |           69.619 (+-0.242)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (270, 268) -> (224, 224) aa=True     |        245.708 (+-0.385)        |           155.630 (+-3.531)            |          156.625 (+-1.033)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (1024, 1024) -> (256, 256) aa=True   |        783.667 (+-5.111)        |           668.360 (+-1.588)            |          663.363 (+-2.076)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (64, 64) -> (224, 224) aa=False      |        166.468 (+-3.751)        |            74.924 (+-0.459)            |           77.107 (+-0.360)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (224, 224) -> (270, 268) aa=False    |        281.174 (+-0.422)        |           152.367 (+-1.417)            |          151.679 (+-1.142)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (1024, 1024) aa=False  |        2916.444 (+-9.280)       |           923.861 (+-7.625)            |         1711.277 (+-79.731)        
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (224, 224) -> (64, 64) aa=False      |         56.718 (+-0.095)        |            47.596 (+-0.099)            |           50.080 (+-0.312)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (270, 268) -> (224, 224) aa=False    |        229.192 (+-0.471)        |           139.786 (+-0.462)            |          138.363 (+-1.158)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (1024, 1024) -> (256, 256) aa=False  |        544.392 (+-0.863)        |           435.130 (+-1.314)            |          422.689 (+-2.519)         

Times are in microseconds (us).
