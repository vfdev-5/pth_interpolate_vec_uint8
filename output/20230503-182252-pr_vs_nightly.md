Description:
- 20230503-172012-pr
Torch version: 2.1.0a0+git5e1bf10
Torch config: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - CPU capability usage: AVX2
  - CUDA Runtime 11.7
  - NVCC architecture flags: -gencode;arch=compute_61,code=sm_61
  - CuDNN 8.5
  - Build settings: BUILD_TYPE=Release, CUDA_VERSION=11.7, CUDNN_VERSION=8.5.0, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOROCTRACER -DUSE_PYTORCH_QNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=1, USE_CUDNN=1, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=0, USE_MPI=OFF, USE_NCCL=ON, USE_NNPACK=0, USE_OPENMP=ON, USE_ROCM=OFF, 


- 20230503-175421-nightly
Torch version: 2.1.0a0+git2b75955
Torch config: PyTorch built with:
  - GCC 9.4
  - C++ Version: 201703
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - CPU capability usage: AVX2
  - Build settings: BUILD_TYPE=Release, CXX_COMPILER=/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=1 -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_PYTORCH_QNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=old-style-cast -Wno-invalid-partial-specialization -Wno-unused-private-field -Wno-aligned-allocation-unavailable -Wno-missing-braces -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.1.0, USE_CUDA=0, USE_CUDNN=OFF, USE_EIGEN_FOR_BLAS=ON, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=OFF, USE_MKLDNN=0, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=0, USE_OPENMP=ON, USE_ROCM=OFF, 



[-------------------------------------------------------------------------------- Resize --------------------------------------------------------------------------------]
                                                                                                   |  torch (2.1.0a0+git5e1bf10) PR  |  torch (2.1.0a0+git2b75955) nightly
1 threads: ---------------------------------------------------------------------------------------------------------------------------------------------------------------
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (32, 32) aa=True       |         60.195 (+-0.110)        |           57.214 (+-0.424)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (32, 32) aa=False      |         41.997 (+-0.063)        |           38.216 (+-0.293)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (224, 224) aa=True     |        238.729 (+-3.730)        |          153.715 (+-1.863)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (224, 224) aa=False    |        227.997 (+-0.673)        |          142.334 (+-1.228)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (320, 320) aa=True     |        378.648 (+-1.138)        |          211.348 (+-1.414)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (320, 320) aa=False    |        376.562 (+-0.851)        |          209.114 (+-1.185)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (520, 520) -> (32, 32) aa=True       |        134.921 (+-0.893)        |          130.253 (+-1.623)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (520, 520) -> (32, 32) aa=False      |         62.643 (+-0.078)        |           58.912 (+-0.239)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (520, 520) -> (224, 224) aa=True     |        421.559 (+-0.871)        |          320.759 (+-1.909)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (520, 520) -> (224, 224) aa=False    |        330.721 (+-0.856)        |          233.737 (+-1.224)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (712, 712) -> (32, 32) aa=True       |        212.597 (+-1.623)        |          200.755 (+-2.345)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (712, 712) -> (32, 32) aa=False      |         81.333 (+-0.354)        |           77.031 (+-0.204)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (712, 712) -> (224, 224) aa=True     |        551.766 (+-1.245)        |          441.706 (+-2.739)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (712, 712) -> (224, 224) aa=False    |        410.380 (+-0.547)        |          305.165 (+-2.064)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (64, 64) -> (224, 224) aa=True       |        157.433 (+-0.624)        |           79.155 (+-0.833)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (224, 224) -> (270, 268) aa=True     |        280.512 (+-0.648)        |          161.004 (+-1.942)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (1024, 1024) aa=True   |        2538.113 (+-9.528)       |          928.546 (+-95.870)        
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (224, 224) -> (64, 64) aa=True       |         80.689 (+-0.350)        |           71.605 (+-0.463)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (270, 268) -> (224, 224) aa=True     |        251.410 (+-0.878)        |          165.562 (+-1.510)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (1024, 1024) -> (256, 256) aa=True   |        900.791 (+-3.442)        |          755.461 (+-5.104)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (64, 64) -> (224, 224) aa=False      |        157.430 (+-1.383)        |           78.324 (+-0.923)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (224, 224) -> (270, 268) aa=False    |        278.127 (+-0.909)        |          159.721 (+-1.506)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (1024, 1024) aa=False  |        2551.858 (+-7.387)       |          1079.129 (+-7.526)        
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (224, 224) -> (64, 64) aa=False      |         61.960 (+-0.093)        |           53.629 (+-0.379)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (270, 268) -> (224, 224) aa=False    |        235.107 (+-0.628)        |          148.498 (+-1.427)         
      3 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (1024, 1024) -> (256, 256) aa=False  |        640.562 (+-2.822)        |          475.083 (+-12.263)        
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (32, 32) aa=True       |         54.212 (+-0.092)        |           53.408 (+-0.271)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (32, 32) aa=False      |         37.343 (+-0.096)        |           36.635 (+-0.216)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (224, 224) aa=True     |        234.217 (+-0.605)        |          145.309 (+-1.309)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (224, 224) aa=False    |        222.839 (+-0.638)        |          133.669 (+-1.325)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (320, 320) aa=True     |        386.129 (+-0.511)        |          197.793 (+-1.805)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (320, 320) aa=False    |        380.939 (+-0.838)        |          195.023 (+-1.411)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (520, 520) -> (32, 32) aa=True       |        125.133 (+-0.820)        |          125.477 (+-0.970)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (520, 520) -> (32, 32) aa=False      |         58.299 (+-0.108)        |           58.206 (+-0.221)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (520, 520) -> (224, 224) aa=True     |        374.934 (+-0.954)        |          286.634 (+-1.454)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (520, 520) -> (224, 224) aa=False    |        304.792 (+-0.925)        |          214.171 (+-1.352)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (712, 712) -> (32, 32) aa=True       |        194.770 (+-1.205)        |          193.727 (+-1.792)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (712, 712) -> (32, 32) aa=False      |         75.657 (+-0.327)        |           75.457 (+-0.396)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (712, 712) -> (224, 224) aa=True     |        513.048 (+-1.171)        |          422.883 (+-2.318)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (712, 712) -> (224, 224) aa=False    |        364.671 (+-0.697)        |          274.941 (+-2.214)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (64, 64) -> (224, 224) aa=True       |        167.698 (+-1.025)        |           78.079 (+-0.249)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (224, 224) -> (270, 268) aa=True     |        282.203 (+-0.460)        |          154.115 (+-1.665)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (1024, 1024) aa=True   |        2889.096 (+-8.170)       |         1194.077 (+-218.266)       
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (224, 224) -> (64, 64) aa=True       |         74.770 (+-1.102)        |           69.733 (+-0.403)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (270, 268) -> (224, 224) aa=True     |        244.946 (+-0.567)        |          156.558 (+-1.310)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (1024, 1024) -> (256, 256) aa=True   |        780.658 (+-27.775)       |          658.042 (+-2.238)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (64, 64) -> (224, 224) aa=False      |        165.605 (+-0.418)        |           77.110 (+-0.279)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (224, 224) -> (270, 268) aa=False    |        280.672 (+-0.959)        |          151.264 (+-0.980)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (256, 256) -> (1024, 1024) aa=False  |       2873.503 (+-11.911)       |          902.011 (+-3.266)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (224, 224) -> (64, 64) aa=False      |         56.264 (+-0.144)        |           51.307 (+-0.366)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (270, 268) -> (224, 224) aa=False    |        228.014 (+-1.457)        |          138.703 (+-1.108)         
      4 torch.uint8 channels_last(squeeze/unsqueeze) bilinear (1024, 1024) -> (256, 256) aa=False  |        546.929 (+-2.022)        |          422.222 (+-2.894)         

Times are in microseconds (us).
