Description:
- 20230412-141243-PT2.0
Torch version: 2.0.0+cpu
Torch config: PyTorch built with:
  - GCC 9.3
  - C++ Version: 201703
  - Intel(R) oneAPI Math Kernel Library Version 2022.2-Product Build 20220804 for Intel(R) 64 architecture applications
  - Intel(R) MKL-DNN v2.7.3 (Git Hash 6dbeffbae1f23cbbeae17adb7b5b13f1f37c080e)
  - OpenMP 201511 (a.k.a. OpenMP 4.5)
  - LAPACK is enabled (usually provided by MKL)
  - NNPACK is enabled
  - CPU capability usage: NO AVX
  - Build settings: BLAS_INFO=mkl, BUILD_TYPE=Release, CXX_COMPILER=/opt/rh/devtoolset-9/root/usr/bin/c++, CXX_FLAGS= -D_GLIBCXX_USE_CXX11_ABI=0 -fabi-version=11 -Wno-deprecated -fvisibility-inlines-hidden -DUSE_PTHREADPOOL -DNDEBUG -DUSE_KINETO -DLIBKINETO_NOCUPTI -DLIBKINETO_NOROCTRACER -DUSE_FBGEMM -DUSE_QNNPACK -DUSE_PYTORCH_QNNPACK -DUSE_XNNPACK -DSYMBOLICATE_MOBILE_DEBUG_HANDLE -O2 -fPIC -Wall -Wextra -Werror=return-type -Werror=non-virtual-dtor -Werror=bool-operation -Wnarrowing -Wno-missing-field-initializers -Wno-type-limits -Wno-array-bounds -Wno-unknown-pragmas -Wunused-local-typedefs -Wno-unused-parameter -Wno-unused-function -Wno-unused-result -Wno-strict-overflow -Wno-strict-aliasing -Wno-error=deprecated-declarations -Wno-stringop-overflow -Wno-psabi -Wno-error=pedantic -Wno-error=redundant-decls -Wno-error=old-style-cast -fdiagnostics-color=always -faligned-new -Wno-unused-but-set-variable -Wno-maybe-uninitialized -fno-math-errno -fno-trapping-math -Werror=format -Werror=cast-function-type -Wno-stringop-overflow, LAPACK_INFO=mkl, PERF_WITH_AVX=1, PERF_WITH_AVX2=1, PERF_WITH_AVX512=1, TORCH_DISABLE_GPU_ASSERTS=ON, TORCH_VERSION=2.0.0, USE_CUDA=0, USE_CUDNN=OFF, USE_EXCEPTION_PTR=1, USE_GFLAGS=OFF, USE_GLOG=OFF, USE_MKL=ON, USE_MKLDNN=ON, USE_MPI=OFF, USE_NCCL=OFF, USE_NNPACK=ON, USE_OPENMP=ON, USE_ROCM=OFF, 



[---------------------------------------------------------------------------------------- Resize ---------------------------------------------------------------------------------------]
                                                                                 |      Pillow (9.5.0)     |  torch (2.0.0+cpu) PT2.0  |     torchvision resize    |  Speed-up: PTH vs TV
4 threads: ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------
      3 torch.uint8 channels_first bilinear (64, 64) -> (224, 224) aa=True       |    1161.438 (+-5.021)   |    2070.279 (+-278.197)   |    2199.172 (+-411.388)   |    1.062 (+-0.000)  
      3 torch.uint8 channels_first bilinear (224, 224) -> (270, 268) aa=True     |   2385.159 (+-18.863)   |    3753.838 (+-296.045)   |    3967.097 (+-573.030)   |    1.057 (+-0.000)  
      3 torch.uint8 channels_first bilinear (256, 256) -> (1024, 1024) aa=True   |  22418.360 (+-155.276)  |   24684.685 (+-2328.687)  |   44760.830 (+-2338.789)  |    1.813 (+-0.000)  
      3 torch.uint8 channels_first bilinear (224, 224) -> (64, 64) aa=True       |    771.304 (+-12.648)   |    1175.675 (+-108.534)   |    1876.239 (+-313.336)   |    1.596 (+-0.000)  
      3 torch.uint8 channels_first bilinear (270, 268) -> (224, 224) aa=True     |   2421.832 (+-38.422)   |    3693.946 (+-323.162)   |    4077.606 (+-519.560)   |    1.104 (+-0.000)  
      3 torch.uint8 channels_first bilinear (256, 256) -> (224, 224) aa=True     |   2272.879 (+-17.042)   |    3599.072 (+-308.874)   |    3759.007 (+-555.049)   |    1.044 (+-0.000)  
      3 torch.uint8 channels_first bilinear (1024, 1024) -> (256, 256) aa=True   |  14575.356 (+-275.280)  |   14514.828 (+-1583.701)  |   18056.410 (+-1642.454)  |    1.244 (+-0.000)  
      3 torch.uint8 channels_first bilinear (64, 64) -> (224, 224) aa=False      |                         |    2006.106 (+-266.245)   |    4186.242 (+-496.524)   |    2.087 (+-0.000)  
      3 torch.uint8 channels_first bilinear (224, 224) -> (270, 268) aa=False    |                         |    3655.054 (+-328.735)   |    5902.464 (+-580.066)   |    1.615 (+-0.000)  
      3 torch.uint8 channels_first bilinear (256, 256) -> (1024, 1024) aa=False  |                         |   24662.329 (+-2496.422)  |   79244.389 (+-2627.873)  |    3.213 (+-0.000)  
      3 torch.uint8 channels_first bilinear (224, 224) -> (64, 64) aa=False      |                         |     885.135 (+-82.131)    |    1701.482 (+-446.010)   |    1.922 (+-0.000)  
      3 torch.uint8 channels_first bilinear (270, 268) -> (224, 224) aa=False    |                         |    3434.543 (+-321.671)   |    4597.465 (+-556.127)   |    1.339 (+-0.000)  
      3 torch.uint8 channels_first bilinear (256, 256) -> (224, 224) aa=False    |                         |    3245.340 (+-299.193)   |    4659.978 (+-566.993)   |    1.436 (+-0.000)  
      3 torch.uint8 channels_first bilinear (1024, 1024) -> (256, 256) aa=False  |                         |    9948.110 (+-492.166)   |   17259.842 (+-1486.700)  |    1.735 (+-0.000)  
      3 torch.uint8 channels_last bilinear (64, 64) -> (224, 224) aa=True        |    1158.137 (+-3.490)   |    2954.896 (+-376.242)   |    3528.245 (+-483.355)   |    1.194 (+-0.000)  
      3 torch.uint8 channels_last bilinear (224, 224) -> (270, 268) aa=True      |   2384.988 (+-13.964)   |    4922.980 (+-352.862)   |    5525.646 (+-586.000)   |    1.122 (+-0.000)  
      3 torch.uint8 channels_last bilinear (256, 256) -> (1024, 1024) aa=True    |  22424.745 (+-345.546)  |   40993.638 (+-3168.632)  |   40653.079 (+-3222.297)  |    0.992 (+-0.000)  
      3 torch.uint8 channels_last bilinear (224, 224) -> (64, 64) aa=True        |    958.246 (+-69.478)   |    1609.529 (+-90.227)    |    2210.745 (+-214.632)   |    1.374 (+-0.000)  
      3 torch.uint8 channels_last bilinear (270, 268) -> (224, 224) aa=True      |   2994.045 (+-52.502)   |    5139.361 (+-246.665)   |    5315.607 (+-527.112)   |    1.034 (+-0.000)  
      3 torch.uint8 channels_last bilinear (256, 256) -> (224, 224) aa=True      |   2804.541 (+-38.886)   |    5103.128 (+-320.630)   |    5145.396 (+-422.129)   |    1.008 (+-0.000)  
      3 torch.uint8 channels_last bilinear (1024, 1024) -> (256, 256) aa=True    |  18035.657 (+-123.132)  |   19756.529 (+-1743.070)  |   22218.457 (+-1772.135)  |    1.125 (+-0.000)  
      3 torch.uint8 channels_last bilinear (64, 64) -> (224, 224) aa=False       |                         |    3303.501 (+-267.370)   |    5620.020 (+-428.549)   |    1.701 (+-0.000)  
      3 torch.uint8 channels_last bilinear (224, 224) -> (270, 268) aa=False     |                         |    5789.249 (+-264.209)   |    7850.477 (+-448.321)   |    1.356 (+-0.000)  
      3 torch.uint8 channels_last bilinear (256, 256) -> (1024, 1024) aa=False   |                         |   50476.069 (+-3324.810)  |  116164.765 (+-3572.311)  |    2.301 (+-0.000)  
      3 torch.uint8 channels_last bilinear (224, 224) -> (64, 64) aa=False       |                         |    1152.213 (+-47.431)    |    991.313 (+-159.023)    |    0.860 (+-0.000)  
      3 torch.uint8 channels_last bilinear (270, 268) -> (224, 224) aa=False     |                         |    4935.385 (+-272.005)   |    6081.713 (+-436.962)   |    1.232 (+-0.000)  
      3 torch.uint8 channels_last bilinear (256, 256) -> (224, 224) aa=False     |                         |    4706.655 (+-235.813)   |    5882.973 (+-417.491)   |    1.250 (+-0.000)  
      3 torch.uint8 channels_last bilinear (1024, 1024) -> (256, 256) aa=False   |                         |   13289.821 (+-458.713)   |   12258.655 (+-1115.548)  |    0.922 (+-0.000)  

Times are in microseconds (us).
